{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\r\n",
    "ggroup 5_\r\n",
    "\r\n",
    "_Authors: Vignesh, Dustin, Aidan, Javairia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.zip\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files\n",
    "\n",
    "files_to_dl = [\"data.zip\"]  # feel free to add other files here\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        print(file['name'])\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Combining the Data with Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select column names\n",
    "use_cols = ['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "\n",
    "# Get extension for all files\n",
    "all_files = \"../data/*NSW.csv\"\n",
    "\n",
    "# Combine all files\n",
    "ddf = dd.read_csv(all_files, assume_missing=True, usecols=use_cols, include_path_column=True)\n",
    "\n",
    "# Create model column\n",
    "ddf['model'] = ddf['path'].str.split(\"/\", expand=True, n=10)[10].str.split(\"_\", expand=True, n=3)[0]\n",
    "\n",
    "# Drop path column\n",
    "ddf.drop(['path'], axis=1)\n",
    "\n",
    "# Write combined data to single file\n",
    "ddf.to_csv(\"../data/combined_NSW.csv\", single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Loading the combined CSV to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(column, file = \"../data/ACCESS-CM2_daily_rainfall_NSW.csv\"):\n",
    "    counts = pd.Series(dtype=int)\n",
    "\n",
    "    for chunk in pd.read_csv(file, chunksize=10_000):\n",
    "        counts = counts.add(chunk[column].value_counts(), fill_value=0)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_mem = %memit -o get_counts('lat_max')\n",
    "chunk_time = %timeit -o get_counts('lat_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_mem = %memit -o dd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\")['lat_max'].value_counts().compute()\n",
    "dask_time = %timeit -o dd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\")['lat_max'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading only columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset_mem = %memit -o pd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\", usecols=['lat_max'])['lat_max'].value_counts()\n",
    "col_subset_time = %timeit -o pd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\", usecols=['lat_max'])['lat_max'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading with `low_memory=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_mem = %memit -o pd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\", usecols=['lat_max'],low_memory=True)['lat_max'].value_counts()\n",
    "low_time = %timeit -o pd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\", usecols=['lat_max'],low_memory=True)['lat_max'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = pd.DataFrame({\n",
    "    \"Method\": [\"Chunking\", \"Dask\", \"Subsetting Columns\", \"Low Memory\"],\n",
    "    \"Time\": [np.mean(chunk_time.all_runs), np.mean(dask_time.all_runs), np.mean(col_subset_time.all_runs), np.mean(low_time.all_runs)],\n",
    "    \"Memory\": [chunk_mem.mem_usage[0], dask_mem.mem_usage[0], col_subset_mem.mem_usage[0], low_mem.mem_usage[0]]\n",
    "})\n",
    "\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sc = ax.scatter(analysis.Time, analysis.Memory, c = pd.Categorical(analysis.Method).codes, cmap='Dark2')\n",
    "ax.legend(sc.legend_elements()[0], analysis.Method, title=\"Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "From the experiments, The slowest was chunking. This makes sense since we would have to do multiple iterations to get through the entire data. However, the trade off was the amount of memory used: Chunking used the least amount of memory among all methods tested. \n",
    "\n",
    "The fastest method was loading only the column of interest. This was faster than using Dask. The use of `low_memory=True` did not affect the memory usage by much. If we were to pick one of these methods, it would be the column sub-setting method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
