{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\r\n",
    "ggroup 5_\r\n",
    "\r\n",
    "_Authors: Vignesh, Dustin, Aidan, Javairia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.zip\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# code adapted from source 4 \n",
    "\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files\n",
    "\n",
    "files_to_dl = [\"data.zip\"]  # feel free to add other files here\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        print(file['name'])\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Combining the Data with Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select column names\n",
    "use_cols = ['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "\n",
    "# Get extension for all files\n",
    "all_files = \"../data/*NSW.csv\"\n",
    "\n",
    "# Combine all files\n",
    "ddf = dd.read_csv(all_files, assume_missing=True, usecols=use_cols, include_path_column=True)\n",
    "\n",
    "# Create model column\n",
    "ddf['model'] = ddf['path'].str.split(\"/\", expand=True, n=10)[10].str.split(\"_\", expand=True, n=3)[0]\n",
    "\n",
    "# Drop path column\n",
    "ddf.drop(['path'], axis=1)\n",
    "\n",
    "# Write combined data to single file\n",
    "ddf.to_csv(\"../data/combined_NSW.csv\", single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Loading the combined CSV to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(column, file = \"../data/combined_NSW.csv\"):\n",
    "    counts = pd.Series(dtype=int)\n",
    "\n",
    "    for chunk in pd.read_csv(file, chunksize=10_000):\n",
    "        counts = counts.add(chunk[column].value_counts(), fill_value=0)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_mem = %memit -o get_counts('lat_max')\n",
    "chunk_time = %timeit -o get_counts('lat_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_mem = %memit -o dd.read_csv(\"../data/combined_NSW.csv\")['lat_max'].value_counts().compute()\n",
    "dask_time = %timeit -o dd.read_csv(\"../data/combined_NSW.csv\")['lat_max'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading only columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset_mem = %memit -o pd.read_csv(\"../data/combined_NSW.csv\", usecols=['lat_max'])['lat_max'].value_counts()\n",
    "col_subset_time = %timeit -o pd.read_csv(\"../data/combined_NSW.csv\", usecols=['lat_max'])['lat_max'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading with `low_memory=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_mem = %memit -o pd.read_csv(\"../data/combined_NSW.csv\", usecols=['lat_max'],low_memory=True)['lat_max'].value_counts()\n",
    "low_time = %timeit -o pd.read_csv(\"../data/combined_NSW.csv\", usecols=['lat_max'],low_memory=True)['lat_max'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = pd.DataFrame({\n",
    "    \"Method\": [\"Chunking\", \"Dask\", \"Subsetting Columns\", \"Low Memory\"],\n",
    "    \"Time\": [np.mean(chunk_time.all_runs), np.mean(dask_time.all_runs), np.mean(col_subset_time.all_runs), np.mean(low_time.all_runs)],\n",
    "    \"Memory\": [chunk_mem.mem_usage[0], dask_mem.mem_usage[0], col_subset_mem.mem_usage[0], low_mem.mem_usage[0]]\n",
    "})\n",
    "\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sc = ax.scatter(analysis.Time, analysis.Memory, c = pd.Categorical(analysis.Method).codes, cmap='Dark2')\n",
    "ax.legend(sc.legend_elements()[0], analysis.Method, title=\"Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "From the experiments, The slowest was chunking. This makes sense since we would have to do multiple iterations to get through the entire data. However, the trade off was the amount of memory used: Chunking used the least amount of memory among all methods tested. \n",
    "\n",
    "The fastest method was loading only the column of interest. This was faster than using Dask. The use of `low_memory=True` did not affect the memory usage by much. If we were to pick one of these methods, it would be the column sub-setting method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.read_csv(\"../data/combined_NSW.csv\", usecols=['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Transfer from Python to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(tidyverse)\n",
    "library(here)\n",
    "library(feather)\n",
    "library(arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R -i merge \n",
    "\n",
    "# transfer dataframe to R as python dataframe\n",
    "\n",
    "start_time <- Sys.time()\n",
    "result <- merge %>% count(model)\n",
    "print(result)\n",
    "end_time <- Sys.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# write the dataframe to feather format \n",
    "\n",
    "feather.write_feather(merge, 'data/final_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# write the dataframe to arrow and then parquet format\n",
    "# code adapted from source 1\n",
    "\n",
    "table = pa.Table.from_pandas(merge)\n",
    "pq.write_table(table, 'data/final_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# write the dataframe to arrow format \n",
    "# code adapted from source 4 \n",
    "\n",
    "final_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R \n",
    "\n",
    "# transfer dataframe to R as a feather\n",
    "\n",
    "file_path = here(\"data\", \"final_data.feather\")\n",
    "start_time <- Sys.time()\n",
    "df <- read_feather(file_path)\n",
    "result <- df %>% count(model)\n",
    "print(result)\n",
    "end_time <- Sys.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R \n",
    "\n",
    "# transfer dataframe to R as a parquet \n",
    "\n",
    "file_path = here(\"data\", \"final_data.parquet\")\n",
    "start_time <- Sys.time()\n",
    "df <- read_parquet(file_path)\n",
    "result <- df %>% count(model)\n",
    "print(result)\n",
    "end_time <- Sys.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R -i final_table\n",
    "\n",
    "# transfer dataframe to R from arrow \n",
    "# code adapted from source 4\n",
    "\n",
    "start_time <- Sys.time()\n",
    "result <- final_table %>% collect() %>% count(model)\n",
    "print(result)\n",
    "end_time <- Sys.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "du -sh data/final_data.feather\n",
    "du -sh data/final_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**\n",
    "\n",
    "From the exploration of the different memory and time usage, we can see that parquet store less memory then the feather files as inferred from the shell command above that tells us the directories' usage. From our research, we infer that this is due to the use of dictionary encoding and certain compressions that make this possible (2). Furthermore, we noticed the time it takes to do a basic query is much faster for parquet and feather files than passing the pandas dataframe through pandas exchange because it may be reading all the rows of the file to get the answer to our simple count query. However, parquet files store the metadata of the file and can easily access the columns since they are stored in a columnar format (3) and read the files without having to loop through everything. However, the time difference between feather and parquet files is minimal. We hypothesize that this may be due to the fact that our files are not large enough for us to see the difference. We have also considered using an arrow exchange to go from Python to R. In general, arrow is great for in-memory computing (5) and we noticed it was faster than parquet and feather files. It is also less expensive to write than the parquet file format (5). We have also found that parquet and arrow files are used together as a way of performing many operations in the arrow format and then storing the file as a parquet for long term archival storage (5). Both parquet and arrow files are easily integratable to Spark which will be used a tool for later milestones (2 & 5). But this arrow exchange only has select operations that can be performed in R and this process is still in development as mentioned in our 525 lecture 2 (4). Overall, the conversion to parquet file was easy, simple and the fastest while allowing us to stick to the normal operations in R to read the data into a dataframe again so because of this and the fact that we don't anticipate no additional data that will be added at a later date, we will be using parquet as our final choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "1. https://stackoverflow.com/questions/41066582/python-save-pandas-data-frame-to-parquet-file\n",
    "2. https://stackoverflow.com/questions/48083405/what-are-the-differences-between-feather-and-parquet\n",
    "3. https://luminousmen.com/post/big-data-file-formats\n",
    "4. https://github.ubc.ca/MDS-2020-21/DSCI_525_web-cloud-comp_students\n",
    "5. https://stackoverflow.com/questions/56472727/difference-between-apache-parquet-and-arrow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
